{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e7c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caaf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d4af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for displaying result\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f4bf8",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "1. Initialize replay memory <br>\n",
    "2. Initialize action-value function Q with random weights <br>\n",
    "3. Initialize target action-value function Q' with the same weights as Q<br>\n",
    "4. For each episode:\n",
    "    a. Reset environment to initial state<br>\n",
    "    b. For each time step:<br>\n",
    "    i. With probability epsilon select a random action, otherwise <br>select the action with the highest Q-value<br>\n",
    "    ii. Execute the selected action in the environment and observe<br> the next state and reward<br>\n",
    "    iii. Store the transition in replay memory<br>\n",
    "    iv. Sample a random mini-batch of transitions from replay memory<br>\n",
    "    v. Calculate the target Q-value for each transition<br>\n",
    "    vi. Update the weights of the Q function using the mini-batch of transitions and the calculated target Q-values<br>\n",
    "    vii. Every C steps, update the target Q function by copying the weights from Q to Q'<br>\n",
    "    c. End episode when environment is done<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aff145",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3a946",
   "metadata": {},
   "source": [
    "\n",
    "This is a data structure to store the experience tuples (state, action, reward, next state) \n",
    "that the agent observes. The size of the replay memory is typically limited, and old experiences are \n",
    "replaced by new ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c54ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.memory = []\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        self.memory.append(data)\n",
    "        # replace old experience, if memory is full\n",
    "        if (len(self.memory) > self.capacity):\n",
    "            self.memory.pop(0) \n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef80027d",
   "metadata": {},
   "source": [
    "### Action-value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71737140",
   "metadata": {},
   "source": [
    "This is a neural network that takes a state as input and outputs the Q-value for each action. The weights of the neural network are initialized randomly.\n",
    "\n",
    "They are of 2 types:\n",
    "\n",
    "1. Action-value function Q \n",
    "2. Target Action-value function Q':\n",
    "   This is a copy of the Q neural network with the same weights. It is used to calculate the target Q-values for the update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264ddab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# as q` gets its weight from q, let's make a utility update function\n",
    "\n",
    "def update_weights(q, q_):\n",
    "    q_.load_state_dict(q.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b7a53",
   "metadata": {},
   "source": [
    "#### Description\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "#### Action Space\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "0: Push cart to the left\n",
    "1: Push cart to the right\n",
    "\n",
    "`Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it`\n",
    "\n",
    "#### Observation Space\n",
    "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n",
    "\n",
    "\n",
    "##### Observation\n",
    "\n",
    "1. Cart Position\n",
    "\n",
    "2. Cart Velocity\n",
    "\n",
    "3. Pole Angle\n",
    "\n",
    "4. Pole Angular Velocity\n",
    "\n",
    "\n",
    "Note: While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "\n",
    "The cart x-position (index 0) can be take values between (-4.8, 4.8), but the episode terminates if the cart leaves the (-2.4, 2.4) range.\n",
    "The pole angle can be observed between (-.418, .418) radians (or ±24°), but the episode terminates if the pole angle is not in the range (-.2095, .2095) (or ±12°)\n",
    "\n",
    "\n",
    "#### Rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "#### Starting State\n",
    "All observations are assigned a uniformly random value in (-0.05, 0.05)\n",
    "\n",
    "#### Episode End\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "Termination: Pole Angle is greater than ±12°\n",
    "Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "Truncation: Episode length is greater than 500 (200 for v0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3372b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup ENV\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "# observation space\n",
    "# cart position, cart velocity, pole angle, pole angular velocity\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "#action space\n",
    "# left, right\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc8c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v. Calculate the target Q-value for each transition\n",
    "# vi. Update the weights of the Q function using the mini-batch of transitions and the calculated target Q-values\n",
    "\n",
    "def train(batch, dqn, target_dqn, optimizer, gamma):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    Q = dqn(states)\n",
    "    Q = Q.gather(1, actions)\n",
    "    Q_next = target_dqn(next_states).max(1)[0].detach()\n",
    "    target = rewards + (1 - dones) * gamma * Q_next.unsqueeze(1)\n",
    "    loss = F.mse_loss(Q, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff420805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "replay_capacity = 10000\n",
    "batch_size = 3\n",
    "num_episodes = 1000\n",
    "max_steps = 500\n",
    "\n",
    "# Initialize replay memory\n",
    "replay = ReplayMemory(replay_capacity)\n",
    "\n",
    "# Initialize action-value function Q with random weights\n",
    "dqn = QNet(state_size, action_size).to(device)\n",
    "dqn_ = QNet(state_size, action_size).to(device)\n",
    "\n",
    "# Initialize target action-value function Q' with the same weights as Q\n",
    "update_weights(dqn, dqn_)\n",
    "\n",
    "# optimizer \n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
    "target_update = 10\n",
    "gamma = 0.99 # determines the importance of next steps rewards\n",
    "\n",
    "# epsilon\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "# For each episode:\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # a. Reset environment to initial state\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    # b. For each time step\n",
    "    for t in range(max_steps):\n",
    "        # i. With probability epsilon select a random action, \n",
    "        #    otherwise select the action with the highest Q-value\n",
    "        if epsilon > random.random():\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = dqn(torch.FloatTensor(state)).argmax().item()\n",
    "        \n",
    "        # ii. Execute the selected action in the environment \n",
    "        # and observe the next state and reward\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        # iii. Store the transition in replay memory\n",
    "        replay.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # step v and vi is training the neural network\n",
    "        # executing this in separate function\n",
    "        if len(replay) > batch_size:\n",
    "            # iv. Sample a random mini-batch of transitions from replay memory\n",
    "            batch = replay.get_batch(batch_size)\n",
    "            states, actions, rewards, next_states, dones = batch\n",
    "            train(batch, dqn, dqn_, optimizer, gamma)\n",
    "            \n",
    "        # vii. Every C steps, update the target Q function \n",
    "        # by copying the weights from Q to Q'\n",
    "        \n",
    "        if t % target_update == 0:\n",
    "            update_weights(dqn, dqn_)\n",
    "        \n",
    "        print(score)\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow( env.render() )\n",
    "        plt.show()\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # track scores\n",
    "\n",
    "    # update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "    \n",
    "    # print progress\n",
    "#     print(f\"{episode + 1}/{num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c6c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b53d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57561210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
